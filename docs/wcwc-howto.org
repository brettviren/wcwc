#+title: How to do various things on WCWC.
#+setupfile: wcwc-setup.org

This document collects various recipes to perform some specific tasks.  It assumes basic understanding of WCWC and the ~wcwc~ command (see [[file:wcwc.org]]) and in some cases assumes the BNL WCWC (see [[file:wcwc-bnl.org]]).  If there is something not here that you think would be useful to add, feel free to reach out to the WCWC admin or make an Issue or PR on the [[https://github.com/brettviren/wcwc][WCWC GitHub]].  

* meta :noexport:

#+begin_src elisp :results none
(setenv "PATH" (concat (getenv "PATH") ":" (expand-file-name "../scripts")))
#+end_src

#+begin_src sh :results output drawer
scp wcwc-howto.html wcwc-howto.pdf hierocles.phy.bnl.gov:public_html/wire-cell/docs/
#+end_src

#+RESULTS:
:results:
:end:

* Listing packages inside a Spack Environment
:PROPERTIES:
:CUSTOM_ID: find-in-env
:END:


When a Spack Environment is "activated", Spack sees only the subset of packages
that have been added to the environment when it was constructed.  Thus the usual ~find~ command may be used.

#+begin_example
$ wcwc shell -e <env-name-or-dir>
$ wcwc find
#+end_example

* Use PyTorch

** The ~py-torch~ spec

In particular ~py-torch~ can require variants to be specified.  In particular with or without CUDA and in some cases the CUDA compute capability (aka CC aka ~cuda_arch~) must be included.  Some typical, though partially specified variants are:

- ~py-torch~cuda~ :: CPU-only build
- ~py-torch+cuda~ :: CPU+GPU build
- ~py-torch+cuda cuda_arch=89~ :: CPU+GPU specifying the nVidia compute capability (CC) version 

Not all CC versions may be available.  CC 8.9 (~89~) covers a large range of hardware (up to 40xx) and software versions (up to 11.x).  See [[file:wcwc-packages.org::#cuda]] for some details.

To see all package instances that are available (but note section [[#find-in-env]]).
#+begin_example
$ wcwc find -lv py-torch
#+end_example
#+begin_src sh :exports results :results output :wrap example
wcwc find -lv py-torch
#+end_src

#+RESULTS:
#+begin_example
-- linux-debian12-x86_64 / gcc@12.2.0 ---------------------------
2dj527f py-torch@2.4.0~caffe2+cuda+cudnn~custom-protobuf~debug+distributed+fbgemm+gloo+kineto~magma~metal+mkldnn+mpi~nccl+numa+numpy+onnx_ml+openmp+qnnpack~rocm+tensorpipe~test~ucc+valgrind+xnnpack build_system=python_pip cuda_arch=89
#+end_example
Specifying an exact instance with the hash may be a useful shorthand: ~py-torch/2dj527f~.


** Environment Module

To use an installed version of ~py-torch~ via an Environment Module:
#+begin_example
$ wcwc shell -l 'py-torch+cuda'
$ python -c 'import torch; print(torch.cuda.is_available())'
True
$ python -c 'import torch; print(torch.cuda.device_count())'
2
#+end_example

** Spack Environment

A Spack Environment with an added instance of ~py-torch~ can be made like:
#+begin_example
$ wcwc env -e py-torch-env "py-torch+cuda cuda_arch=89"
--> In WCWC, activate with: wcwc shell -e wcwc-env

$ wcwc shell -e wcwc-env
$ python -c 'import torch; print(torch.cuda.is_available())'
True
$ python -c 'import torch; print(torch.cuda.device_count())'
2
#+end_example


* Use Spack View to develop WCT
:PROPERTIES:
:CUSTOM_ID: spack-view-wct
:END:

The kernel of a Spack Environment is a Spack View.  This is simply a directory that is the union of all files of the packages from which it was constructed.

** Determine seeds

Views are populated starting from one or more "seed" package instances.  We may identify an installed instance of WCT that best matches the intent of our development with the usual ~wcwc find~:

#+begin_example
$ wcwc find -lv wire-cell-toolkit
-- linux-debian12-x86_64 / clang@17.0.6 -------------------------
jh6gdsn wire-cell-toolkit@0.28.0~cppjsonnet+cuda+emacs+glpk+hdf+root+tbb+torch build_system=generic cuda_arch=89 cxxstd=17
ghw2if5 wire-cell-toolkit@master~cppjsonnet+cuda~emacs+glpk+hdf+root+tbb+torch build_system=generic cuda_arch=89 cxxstd=17

-- linux-debian12-x86_64 / gcc@12.2.0 ---------------------------
3pzitmq wire-cell-toolkit@master~cppjsonnet+cuda~emacs~glpk~hdf~root+tbb+torch build_system=generic cuda_arch=89 cxxstd=17
q3zolqz wire-cell-toolkit@master~cppjsonnet+cuda~emacs+glpk+hdf+root+tbb+torch build_system=generic cuda_arch=89 cxxstd=17
==> 4 installed packages
#+end_example
Our development will be to fix the "hio" package which requires HDF5 so select an instance with ~+hdf~ (~/q3z0lqz~).

** Create the view
We create the view with the following command.
#+begin_example
$ view -S wirecell -e wire-cell-toolkit -d hio-fix -s wire-cell-toolkit/q3zolqz hio-fix/local
#+end_example
The explanation of the arguments:
- ~-S/--stacks~ names any WCWC software stacks providing required packages.  We name ~wirecell~ as that is what provides ~wire-cell-toolkit~.
- ~-e/--exclude~ we want to exclude installation of WCT itself as we will be later installing our own from source.  Note, this must be a package name and not a spec.
- ~-i/--ignore-conflicts~ will skip any files common to multiple packages which is usually benign.
- ~-d/--direnv~ primes a direnv ~.envrc~ file under the given directory which may be used to define basic shell environment settings to make use of the view contents.  We'll place this one level above the view directory itself.
- ~./hio-fix/local~ is the view directory.  We name it ~local/~ as its contents resemble what may be found under Unix's conventional ~/usr/local~.

** Use the view

#+begin_example
$ cd hio-fix
$ direnv allow
#+end_example

** Begin development

#+begin_example
$ git clone git@github.com:WireCell/wire-cell-python.git  python
$ git clone git@github.com:WireCell/wire-cell-toolkit.git toolkit
$ git checkout -t -b remove-h5cpp origin/remove-h5cpp

$ cd python && pip install -e . && cd -

$ cd toolkit
$ ./wcb configure build --prefix=$PREFIX \
     --with-cuda=no --with-libtorch=no \
     --boost-mt --boost-libs=$PREFIX/lib \
     --boost-include=$PREFIX/include
$ cd -
#+end_example
The use of ~--prefix~ here instructs ~wcb~ to consider view contents when auto-locating dependencies. The ~PREFIX~ variable is set by the ~.envrc~ that ~wcwc view~ generated.  CUDA-related is disabled and Boost needs some help to find itself.

** Using the development build

To install the development version into the view directory, one may simply run:
#+begin_example
$ ./wcb install
#+end_example
This will allow ~wire-cell~ and libraries to be used based on the ~layout~ directive in the ~.envrc~ file.  As the developer enters an edit-build-test loop, the time needed for ~./wcb install~ is not welcome.  One may instead run WCT "in place" by extending the ~.envrc~ file:
#+begin_example
PATH_add $PWD/toolkit/build/apps
path_add LD_LIBRARY_PATH $PWD/toolkit/build/{apps,aux,cuda,gen,hio,iface,img,pgraph,sig,sigproc,sio,tbb,util,root,clus}
#+end_example
Edit the ~LD_LIBRARY_PATH~ to taste.  To setup for using ~bats~:
#+begin_example
export BATS_LIB_PATH=$PWD/toolkit/test
PATH_add $PWD/toolkit/test/bats/bin
#+end_example
And, to let WCT locate configuration files:
#+begin_example
path_add WIRECELL_PATH $PWD/toolkit/cfg
path_add WIRECELL_PATH $HOME/opt/wire-cell-data
#+end_example
Note, when [[https://github.com/WireCell/wire-cell-spack/issues/14][this issue]] is fixed, we may locate ~wire-cell-data~ with a more portable path. 

#+begin_note
When rerunning ~wcwc view~ the ~.envrc~ file will be moved aside and rewritten.  Take care not to lose any customization.
#+end_note

* Add existing package to a Spack Environment

The ~env~ command can simply be rerun.

#+begin_example
$ wcwc env -e test-env
$ wcwc env -e test-env zstd
$ wcwc shell -e test-env "zstd --version"
#+end_example

* Install a novel package to a Spack Environment

t.b.d. (currently broken)
