#+title: The Wire-Cell Workstation Cluster
#+setupfile: ~/org/setup.org
#+setupfile: ~/org/setup-topic.org
#+setupfile: ~/org/setup-readtheorg.org
#+options: toc:t

* meta :noexport:

#+begin_src elisp :results none
(setenv "PATH" (concat (getenv "PATH") ":" (expand-file-name "../scripts")))
#+end_src

#+begin_src sh :results output drawer
scp wcwc.html wcwc.pdf hierocles.bnl:public_html/wire-cell/docs/
#+end_src

#+RESULTS:
:results:
:end:

* Introduction
:PROPERTIES:
:CUSTOM_ID: intro
:END:

This document describes a loosely federated cluster of workstations designed to achieve a few high level goals:

- Enable easy sharing of workstation hardware.

- Share common software stacks and simplify their build and distribution.

- Provide easy means to use and develop software based on these stacks.

The name Wire-Cell Workstation Cluster (WCWC) is given to reflect the fact that its need was recognized in the context of development of Wire-Cell software.  Despite that name, it is general to many pursuits.  Indeed, and as will be shown, any software stacks that may be built with Spack can be incorporated into a WCWC.
The WCWC was also conceieved for use on the internal campus network at BNL.  However, it may be replicated in whole or in parts at other institutions or even on personal workstations and laptops.

** Overview
:PROPERTIES:
:CUSTOM_ID: overview
:END:


The following topics are covered in this document:

- Workstation enrollment and user authentication in section [[#enroll]].  This section is BNL-specific.  It may be used as guidance for replicating at another institution or may be abandoned in favor of other methods, including none in the case of a stand-alone workstation or laptop that makes use of WCWC software stack support.

- End-user interaction with the WCWC software ecosystem is described in section [[#user]].  This gives simple guidance on how a user may find what software is available, configure the user's account and use the software.

- Software developer interaction with the WCWC software ecosystem is described in section [[#devel]].  This shows how to develop software leveraging the WCWC ecosystem.

- Administrative tasks such as initializing WCWC software ecosystem from a green field, installing software packages and other such things are in section [[#admin]].

- User, developer and administrator interaction with WCWC software ecosystem is through a single command called ~wcwc~.  The section [[#nabs]] documents how ~wcwc~ works under the hood to automate various tasks.

** Terms
:PROPERTIES:
:CUSTOM_ID: terms
:END:


We will describe "primary" and "secondary" user accounts.  These labels do not
indicate status but simply a "primary" user account is the account on the
workstation that hosts the physical disk providing the user's ~$HOME~.  When a
user logs into another workstation we call that a "secondary" account.

We will also speak of "local" and "remote" accounts.  In all cases a "local" account is the one hosted by the physical computer at which the user happens to be seated.  We will also speak of "internal" account which is when a user is accessing a computer in the local network on which the WCWC workstations communicate.  Any account on a different network is called "external".


Here we will use the term "primary" to indicate a user account on the single workstation that they "own" and which provides the user's physical ~$HOME~ directory.  We describe an non-primary account as "secondary".  A secondary account is simply one used by a user on a WCWC workstation that does not directly provide their physical ~$HOME~ disk.  Any account on a workstation outside of the WCWC is called "external".  An external account may be on a workstation the same LAN as the WCWC or be separated from the WCWC workstation LAN via a firewall.  When a workstation is called "local" it refers to the machine at which a user is physically sitting while a workstation is considered "remote" from the user when it is otherwise accessed.

* Enrollment
:PROPERTIES:
:CUSTOM_ID: enroll
:END:

** BNL WCWC Enrollment

A workstation and user enrollment process is required for hardware and human to become members of BNL WCWC.

*** Workstation enrollment
Enrolling a workstation is initiated by its BNL custodian ("owner") or designee.

- Contact the BNL WCWC admin to discuss any special needs for the workstation or user(s).
- Install a supported OS (see section [[#os]])
- Configure the OS to use a static IP address that has been allocated from BNL ITD.
- Place a copy of the [[https://www.phy.bnl.gov/~bviren/wire-cell/docs/wcwc.pub][BNL WCWC admin SSH public]] keys in ~/root/.ssh/authorized_keys~.

*** User enrollment
Every BNL WCWC user must perform these tasks to assure BNL WCWC membership:
- Obtain a [[https://www.phy.bnl.gov/Accounts/][Physics department SSH account]].
- Determine on which workstation they will be a primary user (where their ~$HOME~ resides)
- Coordinate with the WCWC admin make alteration to user IDs and file ownership and permissions (see below).

Different, pre-existing local user accounts tend to have conflicting user ID numbers (eg 1000 or 1001).  A unique user ID is required to simplify the sharing of ~$HOME~ via NFS.  The process to migrate to a unique user ID number is relatively simple and the WCWC admin will handle it.  However, the user must fully log out and cease any running processes on the workstation while the UID change is made.


*** Supported operating systems
:PROPERTIES:
:CUSTOM_ID: os
:END:

BNL WCWC currently supports the Debian version 12 distribution of GNU/Linux (aka "stable" aka "bookworm").  BNL WCWC workstations will be upgraded to Future releases of Debian in coordination with the workstations custodians and WCWC users.

** Other WCWCs

The use of the ~wcwc~ command, described in the sections below, is not strongly dependent on the OS-layer.
Institutions that adopt the WCWC model are encouraged to develop their own enrollment processes and OS-layer policy.  These necessarily are expected to differ than the above.  Likewise, individual users of laptops or non-clustered workstations can benefit from ~wcwc~ and likely need no special "enrollment" process.

The ~wcwc~ command for providing software suites is well tested on the operating systems used by the BNL WCWC.  It may work on other Linux distributions or perhaps even other unix-like operating systems though binaries produced by ~wcwc~ are not expected to be portable.  Future work may be envisioned to aggregate a broader community with a heterogeneous set of operating systems.

* User
:PROPERTIES:
:CUSTOM_ID: user
:END:

** OS packages

In general, only OS packages provided by the distribution (Debian) will be installed.  These can be installed across all WCWC workstations.  Special cases for non-distribution OS packages may be considered. Contact the WCWC admin if some OS package is wanted but not installed.

** Introduction of Spack and ~wcwc~

The large software stacks for various projects and experiments are installed by the WCWC admin via Spack and made available to each WCWC workstation under the ~/wcwc~ directory.   Spack is a system used throughout HEP and other scientific communities to build and manage a large variety of different software from single packages to huge interdependent suites of multiple versions, build variants, compilers and hardware architectures.  All software provided under ~/wcwc~ can be manged and utilized with the ~spack~ command.  On top of ~spack~, WCWC workstations provide the ~wcwc~ command which automates and simplifies common required tasks.  The ~wcwc~ command should be available to all users in their default ~$PATH~.  To get started with ~wcwc~, a user is encouraged to become familiar with its command-line help.

#+begin_example
$ wcwc --help

$ wcwc <command> --help
#+end_example

The remainder of this section describes how to use ~wcwc~ for common tasks that "end users" may require.  Subsequently in  sections [[#devel]] and [[#admin]] we describe how to use ~wcwc~ for developers and cluster administrators, respectively.

** Shell environment

Special shell environment is technically not required to use Spack software but some level of shell environment setting is helpful to hide the necessarily complexity of Spack structure.  Spack provides a few means to make these shell settings and ~wcwc~ simplifies and extends them through the ~wcwc shell~ command.

#+begin_example
$ wcwc shell --help
#+end_example
We will now walk through a series of progressively more sophisticated methods to configure a shell environment.

*** Basic shell environment

A minimal shell environment can be obtained which provides access to ~spack~:

#+begin_example
u$ wcwc shell
s$ exit
#+end_example
This starts a new instance of the user's shell with access to ~spack~.  The user may perform low-level ~spack~ commands in this shell and when finished may simply ~exit~ to return to the original shell environment.

#+begin_note
We have introduced a notation to mark the shell prompt to indicate something about a particular shell configuration.  Here, ~u$~ indicates the user's nominal shell environment and ~s$~ indicates a shell configured for Spack.
#+end_note

By default, ~wcwc shell~ executes an instance of the same shell currently run by the user.  The user may explicitly specify the shell to execute:

#+begin_example
tcsh-u$ wcwc shell --shell bash
bash-s$ 
#+end_example

*** Using individual packages

Spack supports a method to modify shell environment to use individual software packages called Environment Modules and ~wcwc shell~ exposes this support

#+begin_example
u$ wcwc shell --load root
#+end_example
In general, any given package is specified with a description written in a small language called a Spack "spec".  See section [[#spec]] for a brief introduction.

*** One-shot commands

Above we used ~spack spec~ to enter an interactive shell.  The user must issue ~exit~ to return to the original, calling shell.  When the user requires executing a single command in a shell configured to use Spack, they may do so like:

#+begin_example
u$ wcwc shell --load root --command "root -l"
root [0] .q
u$ 
#+end_example
Note, exiting ~root~ brings the user back to the original shell.

*** Using Spack Environments
:PROPERTIES:
:CUSTOM_ID: user-env
:END:

Spack provides a shell setting mechanism called Spack Environments.  These are NOT the same as Environment Modules but achieve some of the same goals.  While Environment Modules will add an entry to ~PATH~-like environment variables for each package "loaded", a Spack Environment aggregates packages through a file system directory called a *view*.  This provides a few benefits: only a single entry is needed in ~PATH~-like variables and the aggregation is durable and usable beyond a particular shell.

Spack Environments can be of two types: "system" aka "shared and "personal" aka "anonymous" aka "developer".  Here, we describe the first type and see section [[#devenv]] for information about the second type.  The "system" environments are defined by the WCWC admin and may be shared by users.  To make use of an shared environment:

#+begin_example
u$ wcwc shell --environment <envname>
e$
#+end_example

Available environments can be listed:
#+begin_example
u$ wcwc list-env --stacks all
u$ wcwc shell -S <stackname> --environment <envname>
#+end_example
As implied here, in WCWC, environments are typically defined in the context of a WCWC "stack".  See section [[#stacks]] for more information on stacks.

*** Combining

Environment Modules and Spack Environments may be used together:
#+begin_example
u$ wcwc shell --load root --environment wct-minimial
r+wc$ 
#+end_example
And one may use either in a one-shot command.
#+begin_example
u$ wcwc shell --load root --environment wct-minimial --command "which root; which wire-cell"
u$
#+end_example

*** Shell mutation

At no point will any ~wcwc shell~ command modify the ....


** A few Spack and WCWC concepts

This section describes some concepts introduced above.  

*** Spack spec
:PROPERTIES:
:CUSTOM_ID: spec
:END:

In the example above, "~root~" is an abstract "spec" naming just the package.  When multiple package *instances* are installed, this name is ambiguous.  The command can not determine which installed instance to select 
and more "concrete" version of the spec must be used.  For example, if there are multiple versions of ~root~ installed we must provide a more complete spec:
#+begin_example
$ wcwc shell --load root@6.30.06
#+end_example

See the Spack documentation for details on [[https://spack.readthedocs.io/en/latest/basic_usage.html#sec-specs][spec]].  In WCWC one can discover installed package instances with:

#+begin_example
u$ wcwc shell --command "spack find root"
-- linux-debian12-x86_64_v3 / gcc@12.2.0 ------------------------
root@6.30.06
==> 1 installed package
#+end_example

In WCWC, packages are installed into the "namespace" of a stack and the above only searches the base Spack stack.  See section [[#stacks]] for more information.

*** WCWC stacks
:PROPERTIES:
:CUSTOM_ID: stacks
:END:


A WCWC "stack" solidifies a nascent Spack pattern.  A "stack" represents a subset of packages from a larger overall set.  Specifically, the subset consists of packages that can be built by a particular Spack "repo".  A WCWC "stack" provides a Spack configuration "scope" that associates a repo, its corresponding Spack "namespace", a package install tree and projection and a location to store definitions of system or shared Spack Environments.  The stack may also have an associated git remote repository URL (and a git branch and/or tag) that provides the Spack "repo".

A stack may coarsely depend on another stack with a stack corresponding to the Spack "builtin" repo forming the trunk of that dependency tree.  If on stack's install tree differs from a dependency then that dependency is considered a Spack "upstream".  Otherwise, stack install trees may coincide though it is recomended to define a stack's "projection" to begin with ~{namespace}/~ in order to keep distinct packages provided by each stack's Spack repo.  In particular, this distinction allows to resolve namespace collisions that arise when different Spack repos providing packages of identical name.

For users of ~wcwc~, what is important to understand is that they may need to be named when using or making Spack Environments and/or packages.  The ~wcwc~ subcommands generally take the ~-S/--stacks~ option to provide a comma-separated list of stacks:
#+begin_example
$ wcwc <command> --stacks wirecell,base [....]
#+end_example



* Developer
:PROPERTIES:
:CUSTOM_ID: devel
:END:

** Personal environments
:PROPERTIES:
:CUSTOM_ID: personal-env
:END:

We saw the use of "system" or "shared" environments in section [[#user-env]].  Here we describe how to make a "personal" environment.  It differs from a system environment largely in that its directory resides in a user-owned area and that we will refer to it by its directory path instead of a name.  It also differs in that it is not shared and its life cycle is solely controlled by user.

To begin, we create a personal environment, based on a number of stacks and potentially with some number of initial specs:
#+begin_example
u$ wcwc make-env --help

u$ wcwc make-env --stacks wirecell --environment my-env/ <spec> [...]
#+end_example
Additional specs may be added by repeating this command listing just the new specs.

We may then create a shell configured to use this environment similarly to how we did with system environments in section [[#user-env]] except that we give our environment directory instead of a name:
#+begin_example
u$ wcwc shell --environment my-env/
e$
#+end_example

** Installing packages to environments
:PROPERTIES:
:CUSTOM_ID: env-install-package
:END:


When adding a spec to an environment as shown in section [[#personal-env]] we must only provide specs that describe packages installed in a stack.  It is also possible to install novel package instances directly to an environment.

Example t.b.d.

** Developer environments
:PROPERTIES:
:CUSTOM_ID: devenv
:END:

Personal environments may be easily used for developing software as all package files are collected into a single file system tree called the "view".  In the above example, the view directory is at:

#+begin_example
my-env/.spack-env/view/
#+end_example

When the environment settings are activated (~wcwc shell -e my-env~) Spack defines a variable ~SPACK_ENV~ that points to the root directory.  This may then be used to configure build systems of software to be developed.  For example using ~waf~ or ~wcb~ from Wire-Cell Toolkit or Prototype we may execute:
#+begin_example
e$ ./wcb configure --prefix=$SPACK_ENV/.spack-env/view [...]
e$ ./wcb install
#+end_example
In the case of WCT, this does two things.  It tells the build system to check in the "prefix" when auto-locating required dependencies and it uses the "prefix" as the installation target.  In the case where the environment and its view are filled with files from WCT itself, this installation merely overwrites the symlinks.


* Administrator
:PROPERTIES:
:CUSTOM_ID: admin
:END:

The WCWC administrator is responsible to assemble the original WCWC, assist with enrollment and populate ~/wcwc~ with packages, stacks, and environments.  Users wishing to make use of ~wcwc~ to supply software to their individual (non-clustered) laptops or workstations will also be an WCWC administrator.  This section describes ways that the ~wcwc~ command can assist in some of these tasks.

** Green field installation

Starting from a "vanilla" computer, we describe how to construct the ~/wcwc~ area.  

#+begin_note
It is recommended to create a special user (eg "wcwc") that has sole write permission to ~/wcwc~.
In principle, multiple users may contribute to modifying ~/wcwc~ but that can lead to confusion and "cruft".
Future WCWC development may explore ways to allow multiple users and workstations to contribute to building out the ~/wcwc~ contents.
#+end_note

We start by selecting a location to store all the software.  Potentially 100s of GB or more may be required depending on how much software is installed.  By default, the ~/wcwc~ directory is used but a non-standard location may be set in one of two ways:
#+begin_example
$ export WCWC_PREFIX=/path/to/wcwc

$ wcwc --prefix=/path/to/wcwc [...]
#+end_example
The command line overrides environment variable.  If a non-standard WCWC prefix is used, then it must be subsequently used by all users of the ~wcwc~ command.  We will assume henceforth that the default prefix is used.

To begin, we use ~wcwc~ to produce an initial configuration file named ~wcwc.yaml~ in the prefix:
#+begin_example
$ wcwc dump-config
#+end_example
Edit this file to change any git tags of stack git repositories and/or add/remove any stacks.  You may also consider redefining the install tree which will hold the installed packages.  Each stack may have its own install tree and/or they may have unique projections via the use of ~{namespace}~ in the pattern.

The base Spack and any other stacks can then be initialized simply with:
#+begin_example
$ wcwc init spack
$ wcwc init wirecell
#+end_example
This creates a ~stacks/<stack>/~ subdirectory under the WCWC prefix to hold the scope and environments and potentially a unique install tree.


** Installing packages

Installing packages with ~wcwc~ requires naming their spec.  Eg:
#+begin_example
$ wcwc install root
#+end_example
This will install the "preferred" version (usually the newest) of the ~root~ package into the default stack (Spack itself with default WCWC configuration).

To install a package into a stack one must name it.  For example, at the time of writing Spack does not build ~go-jsonnet~ but the ~wirecell~ stack does.
#+begin_example
$ wcwc install --stacks wirecell go-jsonnet
#+end_example
When multiple stacks are considered and more than one provide the named package, Spack will determine which to use (the one from the last repo registered).  In such a case, it is recommended to qualify the package name with the namespace.  In a contrived and not realistic example (the two stacks are mutually exclusive at the time of writing):
#+begin_example
$ wcwc install --stacks wirecell,art wirecell.go-jsonnet
#+end_example

As described in section [[#spec]], multiple instances of the same package may be installed.  It is at installation time that one may distinguish versions, variants and other parameters that maybe written in a spec.  For example to install a minimal and a maximal WCT of a specific version:
#+begin_example
$ wcwc install -S wirecell wire-cell-toolkit@0.28.0~glpk~root~hdf
$ wcwc install -S wirecell wire-cell-toolkit@0.28.0+glpk+root+hdf
#+end_example

* Nuts and bolts
:PROPERTIES:
:CUSTOM_ID: nabs
:END:

Here we describe how various ~wcwc~ commands work including give Spack-level equivalents where they exist.

* Specific packages

This section gives information about specific packages.

** CUDA

The main motivation for WCWC is to share workstation GPUs at BNL.   All are currently nVidia and so CUDA support is a must.  nVidia phases out support for old hardware in new software (kernel driver and libraries).  That landscape is complex and described by several things:

- compute capability :: (cc) a number like 9.0 which maps to a set of supported compute features
- hardware family :: eg "turing" is a name that implies one or more cc numbers ([[https://developer.nvidia.com/cuda-gpus][table of device cc]],  [[https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list][table of family cc]])
- driver version :: a number like 525.147.05 which gives the version of the nVidia Linux kernel driver
- toolkit version :: a number like 11.8 or 12.0 which gives the version of CUDA user libraries and implies a minimum driver version.

  #+begin_note
  Debian's ~libcuda~ packages are given a *driver version* number and provide support for multiple *toolkit version* numbers.  Debian 12's ~libcuda1~ package is at version 525.147.05 which provides various libcuda versions in the range 5.0 - 12.0.
  #+end_note

A sampling of BNL GPUs, their amount of RAM and their cc, family and [[https://en.wikipedia.org/wiki/CUDA#GPUs_supported][CUDA toolkit minimum version]] (all are supported up to and including CUDA 12).

|-------------+-------+-----+----------+------|
| Card        | RAM   |  CC | Family   | CUDA |
|-------------+-------+-----+----------+------|
| GTX 750 Ti  | 2 GB  | 5.0 | Maxwell  |  6.5 |
|-------------+-------+-----+----------+------|
| GTX 1050 Ti | 4 GB  | 6.1 | Pascal   |  8.0 |
| GTX 1060    | 6 GB  | 6.1 | Pascal   |      |
|-------------+-------+-----+----------+------|
| GTX 1650    | 4 GB  | 7.4 | Turing   | 10.0 |
| GTX 1660    | 6 GB  | 7.5 | Turing   |      |
| RTX 2080 Ti | 11 GB | 7.5 | Turing   |      |
|-------------+-------+-----+----------+------|
| RTX 3080    | 10 GB | 8.6 | Ampere   | 11.0 |
|-------------+-------+-----+----------+------|
| RTX 4070 Ti | 12 GB | 8.9 | Lovelace | 11.8 |
| RTX 4090    | 24 GB | 8.9 | Lovelace |      |
|-------------+-------+-----+----------+------|


Each workstation must have nVidia kernel driver installed and loaded.  It is
recommended to use Debian's package:
#+begin_example
apt install nvidia-kernel-dkms nvidia-smi
#+end_example
A reboot may be required.  The ~nvidia-smi~ command is used to query your GPU device status.  If it gives an error then the kernel driver is not working.
#+begin_example
wcwc@haiku:~$ nvidia-smi  -L
GPU 0: NVIDIA GeForce GTX 750 Ti (UUID: GPU-b87cc150-ba6d-329e-02e4-7f8ddef6a581)
#+end_example

Spack can compile CUDA from scratch or or use the [[https://spack.readthedocs.io/en/latest/gpu_configuration.html#using-an-external-cuda-installation][system CUDA as an "external" package]] in a custom ~packages.hyaml~ configuration file.  Current Spack ~develop~ branch provides CUDA 11.8.0 and WCWC uses that.
The Spack build of CUDA is relatively quick.

** Torch

PyTorch is the primary DNN toolkit of WCWC users and WCT can make use of libtorch.  Spack's ~py-torch~ package builds the Python interface:

#+begin_example
wcwc install "py-torch+cuda~distributed~valgrind~fbgemm~test cuda_arch=89"
#+end_example
Note: this build initially fails due to no failure to find ~omp.h~ even if =~openmp= variant is specified.  The unhygienic fix is:
#+begin_example
apt install libomp-dev 
#+end_example
...


#+begin_example
{prefix}/lib/python3.11/site-packages/torch/lib
libc10.so  libshm.so  libtorch_cpu.so  libtorch_global_deps.so  libtorch_python.so  libtorch.so
#+end_example
Current WCT needs these libraries from Torch:
#+begin_example
libs = ['torch', 'torch_cpu', 'torch_cuda', 'c10_cuda', 'c10']
#+end_example
Where is ~libc10_cuda.so~?
