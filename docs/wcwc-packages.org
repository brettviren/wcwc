#+title: WCWC Packages
#+setupfile: wcwc-setup.org


* meta :noexport:

#+begin_src sh :results output drawer
scp wcwc-packages.html hierocles.phy.bnl.gov:public_html/wire-cell/docs/
#+end_src

#+RESULTS:
:results:
:end:


* Overview

This note collects information about specific packages, be they provided by Spack or the host OS.  It is intended for both WCWC admins and WCWC users.

* Direnv
:PROPERTIES:
:CUSTOM_ID: direnv
:END:

The recommended way to manage task-specific shell environment settings on WCWC (and in general) is with [[https://direnv.net/][=direnv=]].

In brief, ~direnv~ watches as you ~cd~ to different directories and if you enter a directory that has a special ~.envrc~ file or has a parent with this file then ~direnv~ automatically applies its settings to your shell environment.  When you ~cd~ out of a ~.envrc~ controlled tree the settings are automatically undone.

** Quick start
:PROPERTIES:
:CUSTOM_ID: direnv-example
:END:

Here is a quick example:
#+begin_example
$ mkdir ~/mywork/
$ echo 'export FOO=bar' > mywork/.envrc
$ cd ~/mywork

$ direnv allow

$ echo $FOO
bar

$ cd
$ echo $FOO

$ cd ~/mywork
$ echo $FOO
bar
#+end_example
#+begin_note
A ~direnv allow~ is only required when the ~.envrc~ file is modified and ~direnv~ will alert the user when this authorization is required.
#+end_note

** Hooking
:PROPERTIES:
:CUSTOM_ID: direnv-hook
:END:

For ~direnv~ to work it must be "hooked" into your shell.  On BNL WCWC workstation users, use of ~direnv~ is automatic.  Other users should read: https://direnv.net/docs/hook.html

** The .envrc file
:PROPERTIES:
:CUSTOM_ID: direnv-envrc
:END:

You may add lines to ~.envrc~ which set environment variables.  Complex configuration can be generated by calling ~direnv~ functions and a standard library is provided (see the [[https://direnv.net/man/direnv-stdlib.1.html][direnv-stdlib(1) man page]]).

#+begin_note
There is no support to define shell functions or "aliases".  However, an environment variable may be set to hold a command line.  Eg: ~export list='ls -l'~ and the user may run it with ~$list~
#+end_note


For example, when ~wcwc view -d ...~ is used to create a "view" (see [[file:wcwc-howto.org::#spack-view-wct]] for an example) a ~.envrc~ file is created with one line that generates setting for many ~PATH~-like variables:
#+begin_example
load_prefix viewdir
#+end_example

#+begin_note
While ~.envrc~ file may look like ~bash~ syntax, it is not and no shell should ever be made to "source" it.
#+end_note


** Python support
:PROPERTIES:
:CUSTOM_ID: direnv-python
:END:

A common situation is a user wishes to install some Python packages with ~pip install~ but wishes to avoid having ~pip~ download and possibly compile large complex packages.  This scenario is supported with ~direnv~ with one ~.envrc~ line:

#+begin_example
# in .envrc
layout python3 --system-site-packages
#+end_example

After a ~cd~ to a directory controlled by this ~.envrc~ file the user might do:

#+begin_example
$ pip install torch
Requirement already satisfied: torch in /usr/lib/python3/dist-packages (1.13.0a0+gitunknown)
$ python -c 'import torch; print(torch.cuda.is_available())'
False
#+end_example
Unfortunately, as shown, Debian's ~python3-torch~ package does not include CUDA support.


* Python
:PROPERTIES:
:CUSTOM_ID: python
:END:

There are several options for using Python and Python packages on WCWC workstations.  
One may use ~python~ (the program) from Debian or from Spack and one may use Python packages from Debian, Spack or as installed by the user in a Python virtual environment.

** Virtual environment
:PROPERTIES:
:CUSTOM_ID: python-venv
:END:

A Python virtual environment (venv) consists of two main elements
- A directory patterned like the conventional ~/usr/local~ holding files from Python and various python packages.
- An ~activate~ script to configure ~PATH~-like environment variables to make use of these files.
A user needing to install Python packages should do so in a venv and not ever via ~--user~ option.

The recommended way to provide a Python venv is via ~direnv~ as described in section [[#direnv-python]].  With more effort, a user may instead create one manually as described here:
#+begin_example
$ python -m venv myvenv
#+end_example
To configure the shell environment:
#+begin_example
$ source myvenv/bin/activate
#+end_example
There exists also ~activate.fish~ and ~activate.csh~ for some non-sh shells.  To undo the environment settings:
#+begin_example
$ deactivate
#+end_example

Similar to the [[#direnv-python][direnv]] example, OS-installed Python packages can be leveraged by creating the venv with:
#+begin_example
$ python -m venv myvenv --system-site-packages
#+end_example

** Mixing Spack, Debian and pip packages
:PROPERTIES:
:CUSTOM_ID: python-mixed-sources
:END:


Only certain combinations of Spack, Debian and pip packages can be expected to work.  These are as described along with a ~direnv .envrc~ file to produce the venv.

- Spack and pip but not OS ::  When the shell is configured for Spack Python packages (~wcwc shell -l py-XXX~) then a ~pip install~ in a Python venv will use those packages.  The venv relies on the exact ~wcwc shell~ environment to be repeated for subsequent usage.   This is irrespective of using ~--system-site-packages~ or not but in neither case will any OS packages be leveraged.  Eg, ~wcwc shell -l py-numpy~ and with
  #+begin_example
  layout python
  #+end_example

- No Spack but pip and OS :: When the shell is configured for any Spack packages unrelated to Python (or no Spack at all), then the venv can be used without regards to Spack.  Use of ~--system-site-packages~ will cause ~pip install~ to prefer use of an existing OS package to downloading/building it fresh.
  #+begin_example
  layout python3 --system-site-packages
  #+end_example

- Spack and pip and OS with a hack :: If Python is provided by Spack, system packages can be seen and used by ~pip install~ if ~PYTHONPATH~ is set to ~/usr/lib/python3/dist-packages~.
  #+begin_example
  layout python3
  path_add PYTHONPATH /usr/lib/python3/dist-packages
  #+end_example

- Augment OS with Spack, no pip, no venv :: Spack Python and packages can be mixed with OS Python.
  #+begin_example
  $ wcwc shell -l 'py-torch+cuda cuda_arch=89'
  $ jupyter notebook --no-browser  # from the OS
  #+end_example
  Visit the web and test with ~import torch; print(torch.cuda.device_count())~.  See section [[#jupyter]] for more info about use of Jupyter.

* Wire-Cell Toolkit

For developing WCT against Spack packages see: [[file:wcwc-howto.org::#spack-view-wct]].

* CUDA
:PROPERTIES:
:CUSTOM_ID: cuda
:END:


The main motivation for WCWC is to share workstation GPUs at BNL.   Their GPUS are all currently from nVidia and so CUDA support is a must.

** Versions
:PROPERTIES:
:CUSTOM_ID: cuda-versioning
:END:

The nVidia hardware and CUDA software landscape is a little complex and described by several versions:

- compute capability :: (cc) a number like 9.0 which maps to a set of supported compute features
- hardware family :: eg "turing" is a name that implies one or more cc numbers ([[https://developer.nvidia.com/cuda-gpus][table of device cc]],  [[https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list][table of family cc]])
- driver version :: a number like 525.147.05 which gives the version of the nVidia Linux kernel driver
- toolkit version :: a number like 11.8 or 12.0 which gives the version of CUDA user libraries and implies a minimum driver version.

  #+begin_note
  Debian's ~libcuda~ packages are given a *driver version* number and provide support for multiple *toolkit version* numbers.  Debian 12's ~libcuda1~ package is at version 525.147.05 which provides various libcuda versions in the range 5.0 - 12.0.
  #+end_note

A sampling of BNL GPUs, their amount of RAM and their cc, family and [[https://en.wikipedia.org/wiki/CUDA#GPUs_supported][CUDA toolkit minimum version]] (all are supported up to and including CUDA 12).

|-------------+-------+-----+----------+------|
| Card        | RAM   |  CC | Family   | CUDA |
|-------------+-------+-----+----------+------|
| GTX 750 Ti  | 2 GB  | 5.0 | Maxwell  |  6.5 |
|-------------+-------+-----+----------+------|
| GTX 1050 Ti | 4 GB  | 6.1 | Pascal   |  8.0 |
| GTX 1060    | 6 GB  | 6.1 | Pascal   |      |
|-------------+-------+-----+----------+------|
| GTX 1650    | 4 GB  | 7.4 | Turing   | 10.0 |
| GTX 1660    | 6 GB  | 7.5 | Turing   |      |
| RTX 2080 Ti | 11 GB | 7.5 | Turing   |      |
|-------------+-------+-----+----------+------|
| RTX 3080    | 10 GB | 8.6 | Ampere   | 11.0 |
|-------------+-------+-----+----------+------|
| RTX 4070 Ti | 12 GB | 8.9 | Lovelace | 11.8 |
| RTX 4090    | 24 GB | 8.9 | Lovelace |      |
|-------------+-------+-----+----------+------|


Each workstation must have nVidia kernel driver installed and loaded.  It is
recommended to use Debian's package:
#+begin_example
apt install nvidia-kernel-dkms nvidia-smi
#+end_example
A reboot may be required.  The ~nvidia-smi~ command is used to query your GPU device status.  If it gives an error then the kernel driver is not working.
#+begin_example
wcwc@haiku:~$ nvidia-smi  -L
GPU 0: NVIDIA GeForce GTX 750 Ti (UUID: GPU-b87cc150-ba6d-329e-02e4-7f8ddef6a581)
#+end_example

Receiving this error:
#+begin_example
Cuda error in file '.../file.cu' in line XX : no kernel image is available for execution on the device.
#+end_example
Indicates CUDA code was not compiled to target the device (GPU card) compute capability version.

** CUDA and Spack
:PROPERTIES:
:CUSTOM_ID: cuda-spack
:END:


Spack can compile CUDA from scratch with some caveats:
- The Spack ~cuda~ package will only build a ~stubs/libcuda.so~ and not a "real" ~libcuda.so~.  The stub holds "dummy" implementations of the CUDA API and may be used to satisfy link-time dependencies when building in an environment that lacks a "real" ~libcuda.so~.  On WCWC we will rely on Debian's ~libcuda.so~.
- Some Spack packages care about the CUDA "cc" number.  Eg, they may have internal code switches that depend on the version.  Of those that care, some (like ~kokkos~) can be built with support for exactly one "cc".  The conventional way to communicate the "cc" to these packages is via ~cuda_arch~ variant parameter.




* Torch
:PROPERTIES:
:CUSTOM_ID: torch
:END:


PyTorch is the primary DNN toolkit of WCWC users and WCT can make use of libtorch.  Spack's ~py-torch~ package builds the Python interface:

#+begin_example
  $ TMPDIR=$HOME/.cache/wcwc TEMP=$HOME/.cache/wcwc \
    wcwc install "py-torch+cuda cuda_arch=89"
#+end_example
#+begin_note
FIXME: must set ~TMPDIR~ to avoid ~/tmp~ being filled.  For safety, set this and ~TEMP~ in ~wcwc~, but really, this is a bug in the ~py-torch/package.py~ which should honor the configured ~build_stage~ setting.  Note, this is not required when building in a [[file:wcwc-podman.org][podman container]] (assuming podman has sufficient space in the user's home).

Also, earlier builds failed due to OpenMP being detected in the OS but not sufficient to work. 
~apt install libomp-dev~ was needed.  The above was on a different Debian host.  This also motivates the use of a container to build.  In a minimal Debian 12 podman container, py-torch builds without problems (other than loooooong build time).  The ~wcwc make-builder~ command creates the container.
#+end_note

#+begin_example
ls /wcwc/opt/builtin/linux-debian12-x86_64_v4/gcc-12.2.0/py-torch-2.3.0-sq4psktodzbjq7v4bshow7rmcxwoa6r3/lib/python3.11/site-packages/torch/lib                                                                                                          
libc10_cuda.so	libcaffe2_nvrtc.so  libtorch_cpu.so	    libtorch_cuda.so	     libtorch_python.so
libc10.so	libshm.so	   libtorch_cuda_linalg.so  libtorch_global_deps.so  libtorch.so
#+end_example

Simple test:
#+begin_example
  $ python -c 'import torch; print(torch.cuda.is_available())'
  True
  $ python -c 'import torch; print(torch.cuda.device_count())'
  2
#+end_example

Note, had to load a kernel module:

#+begin_example
root@wcgpu0:~# modprobe nvidia_uvm
#+end_example

#+begin_example
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit+torch+cuda cuda_arch=89
#+end_example

* Ollama
:PROPERTIES:
:CUSTOM_ID: ollama
:END:

The Ollama project provides the program ~ollama~ that exercises LLM models.

** Versions
:PROPERTIES:
:CUSTOM_ID: ollama-versions
:END:


The official [[https://github.com/ollama/ollama/][ollama/ollama]] supports a number of different LLM [[https://github.com/ollama/ollama/?tab=readme-ov-file#model-library][models]].  Some other models of interest require a modified ~ollama~ program.  These variants may not support the same models as the official ~ollama~.  In general, one must only use models that are explicitly supported by a particular ~ollama~ variant.

WCWC provides official and variant ~ollama~ as different Spack packages.  Currently:

- ollama :: From Spack ~builtins~ repo with CUDA support (~spack install ollama+cuda~, added in [[https://github.com/spack/spack/pull/46204][PR]]).
- ollamabin :: The official version.  The "bin" implies the package consists of a [[https://github.com/ollama/ollama/releases][binary release]] and to avoid conflict with the Spack ~ollama~ package.
- ollamalux :: A "hacked" version to run specifically ~aiden_lu/minicpm-v2.6~.  The "lux" portion of the name refers to it originating from [[https://github.com/luixiao0/ollama/releases][luixiao0/ollama binary release]].

Regardless of the package name, the user uses the program named ~ollama~.

** Server
:PROPERTIES:
:CUSTOM_ID: ollama-server
:END:

The ~ollama~ server is started simply:
#+begin_example
$ ollama serve
#+end_example
To stop the server, simply hit ~Ctrl-c~.


** Environment
:PROPERTIES:
:CUSTOM_ID: ollama-env
:END:

To help keep the multiple ~ollama~ variants distinct, the WCWC ~ollamaXXX~ packages will set some environment variables (the ~ollama~ package from Spack, proper sets none) when a user "loads" the package (eg ~wcwc shell -l ollamaXXX~).

- ~OLLAMA_HOST~ :: server IP address and port number.
- ~OLLAMA_TMPDIR~ :: set to ~$HOME/tmp~ to avoid filling ~/tmp~
- ~OLLAMA_MODELS~ :: location to store files for models.

#+begin_note
For other available environment variables, run ~ollama serve --help~.
#+end_note

Multiple clients, possibly run by multiple users, may share the same server.  The user that starts the server may also kill it which of course will impact all clients.  Users are encouraged to coordinate any sharing amongst themselves.  A user may attempt to isolate their server by setting ~OLLAMA_HOST~.

#+begin_note
Users should not leave ~ollama~ server running longer than needed as it consumes GPU memory.
#+end_note

For either server or command line client usage, start a shell like:
#+begin_example
u$ wcwc shell -S wirecell -l ollamaXXX
s$ ollama [...]
#+end_example

Replace ~ollamaXXX~ with one of the package names in the list above.


** Use GPUs
:PROPERTIES:
:CUSTOM_ID: ollama-gpu
:END:

By default, ~ollama serve~ should auto detect GPU(s) as long as it finds the "real" ~libcuda.so~ provided by Debian.  See section [[#cuda-spack]].  If this seems to fail, running with the "debug" message level can help understand the cause:
#+begin_example
$ OLLAMA_DEBUG=true ollama serve
#+end_example
Note, these warnings:
#+begin_example
unable to locate gpu dependency libraries
#+end_example
are red-herrings and will be printed even when the GPU are found and usable.


By default ~ollama serve~ will use the GPUs it finds.  To control which GPUs ~ollama~ may use you may set the ~CUDA_VISIBLE_DEVICES~ environment variable to a comma-separated list of GPU indices or IDs.  Enumeration of GPU indices may not be stable so use IDs to list specific GPUs if stability is required.  Some examples:
#+begin_example
  # use "first" device
  $ CUDA_VISIBLE_DEVICES=0 ollama serve

  # use "first" and "second" device
  $ CUDA_VISIBLE_DEVICES=0,1 ollama serve

  # use specific device(s)
  $ nvidia-smi -L  # note the GPU IDs
  $ CUDA_VISIBLE_DEVICES=GPU-7d6529b0-76ce-2e7f-d96e-2018b58d1f2b ollama serve
#+end_example




** Client
:PROPERTIES:
:CUSTOM_ID: ollama-client
:END:

The ~ollama~ program can also act as a command line client:
#+begin_example
$ ollama run llama3.1
#+end_example
Chat is logged by the server.  To exit, hit ~Ctrl-d~.

The first time a model is run its files will be downloaded by the server.


** Checks
:PROPERTIES:
:CUSTOM_ID: ollama-checks
:END:

You can run various checks to observe the state of the server.  Assuming a subshell configured for the ~ollamaXXX~ package:

#+begin_example
$ ollama ls  # list available models
NAME           	ID          	SIZE  	MODIFIED       
llama3.1:latest	f66fc8dc39ea	4.7 GB	42 seconds ago	

$ ollama ps  # list running models:
NAME           	ID          	SIZE  	PROCESSOR	UNTIL              
llama3.1:latest	f66fc8dc39ea	6.7 GB	100% GPU 	4 minutes from now	

$ nvidia-smi # check actual GPU memory usage
...
|    0   N/A  N/A    531064      C   ...unners/cuda_v12/ollama_llama_server       6142MiB |
...
#+end_example

** Specific models
:PROPERTIES:
:CUSTOM_ID: ollama-variants
:END:

Above we showed examples with the official ~ollama~ program and supported models.  This section collects guidance on variants.  The models described below should not be expected to work with the official ~ollama~ nor should the ~ollama~'s described her be expected to run the officially supported models.

*** lux / MiniCPM v2.6
:PROPERTIES:
:CUSTOM_ID: ollama-lux
:END:

The MiniCPM v2.6 model is claimed to be good for OCR.  It requires a variant ~ollama~ which is provided by the ~ollamalux~ package.  It may be used from two shells like:

#+begin_example
$ wcwc shell -S wirecell -l ollamalux -c "ollama serve"

$ wcwc shell -S wirecell -l ollamalux -c "ollama run aiden_lu/minicpm-v2.6:Q4_K_M"
#+end_example



* Jupyter
:PROPERTIES:
:CUSTOM_ID: jupyter
:END:

This section describes the Jupyter family of applications.  

The Spack packaging for jupyter is rather out of date.  Until there is reason to address that, we rely on Debian packages.

** Web user interface
:PROPERTIES:
:CUSTOM_ID: jupyter-notebook
:END:


Perhaps most users see Jupyter as something to use from a web browser.  For *purely local use*, run:
#+begin_example
$ jupyter notebook    # do not run on a remote account
#+end_example
A browser tab visiting the jupyter server should open.  To control what browser to use, set ~BROWSER~ environment variable.  Do not run this command on a remote workstation (see below for that).  If it is run on BNL WCWC, the default browser will be opened which may be the terminal program ~lynx~ (hit ~q~ to quit that).
#+begin_warning
To jupyter notebook server tends to continue to run.  If running in a terminal, the user may hit ~Ctrl-c~ and answer "y" to shutdown.  If neglected, the user may run ~jupyter notebook stop PORT~.  To discover the port(s) of running server(s) run ~jupyter notebook list~.  Do not leave jupyter running longer than needed as it consumes memory and ports.
#+end_warning

To run jupyter notebook a remote workstation an SSH tunnel must be established to forward local browser connections to the remote jupyter server. 

#+begin_example
$ ssh -L 8888:localhost:8888 wcgpu0.phy.bnl.gov jupyter-notebook --no-browser
[...] http://localhost:8888/?token=9b1e482113f4c807250429c905d0e4d04c8175ba3d1a43b5
#+end_example
You may then click that link (if your terminal supports URLs) or copy-paste the URL to a local browser.
#+begin_note
The literal usage of the URL printed by ~jupyter notebook~ assumes you chose the same port number on both ends of the SSH tunnel and that "localhost" resolves correctly to your local workstation.  If the local and remote ports differ then you must edit the URL to use the local port.
#+end_note
The above command establishes the tunnel assuming the default jupyter port will be available for use.  If another user is using that port jupyter will select the next available higher port number and you will see a message like:
#+begin_example
[...] The port 8888 is already in use, trying another port.
[...] http://localhost:8889/?token=9b1e482113f4c807250429c905d0e4d04c8175ba3d1a43b5
#+end_example
This change in expected port breaks the SSH tunnel.  You must Ctrl-c and select "y" to kill the server and the SSH connection and then reconnect using a different port.  In order to increase the chance to claim an unused port and keep consistent ports for the tunnel, we may specify a port of our choosing:
#+begin_example
$ ssh -L 8998:localhost:8998 wcgpu0.phy.bnl.gov jupyter-notebook --no-browser --port 8998
[...] http://localhost:8998/?token=f33b43123890c20b169a7af168274f9e2998cd38f15a33be
#+end_example

** Console
:PROPERTIES:
:CUSTOM_ID: jupyter-console
:END:


A command line alternative to the Jupyter web UI is the "console".  This can be run on the local or remote workstation without the need to establish an SSH tunnel or use a browser.
#+begin_example
$ jupyter console
In [1]: import matplotlib.pyplot as plt

In [2]: plt.plot([1,2,3], [0,1,0])
Out[2]: [<matplotlib.lines.Line2D at 0x7f2907c68790>]
#+end_example
When showing plots or other graphics an X11 client window will be produced.  When running on a remote workstation, this window will only be visible if X11 forwarding has been enabled.  For SSH configuration guide see, for example, [[file:wcwc-bnl.org::#remote-access]] with attention to the ~ForwardX11~ configuration item.

** Ipython
:PROPERTIES:
:CUSTOM_ID: jupyter-ipython
:END:

Jupyter [[https://docs.jupyter.org/en/latest/projects/architecture/content-architecture.html][originated]] from an older project called ~ipython~, which is similar to ~jupyter console~.  ~ipython~ has some convenience features such as pre-loading ~numpy~ and ~matplotlib~ which makes it a useful alternative:

#+begin_example
$ ipython --pylab
In [1]: plt.plot([1,2,3], [0,1,0])
Out[1]: [<matplotlib.lines.Line2D at 0x7f38adf35cd0>]
#+end_example

** Python environment

Default use of ~jupyter~ (or ~ipython~) will have available any Python packages installed via Debian packages.  The user may require additional packages which can be provided via ~pip~ or Spack in the usual way, prior to running ~jupyter~.  See section [[#python]].

To configure for such running, it is recommended to use [[#direnv][direnv]] with a ~.envrc~ file like:
#+begin_example
$ mkdir env-jupyter
$ echo 'layout python3 --system-site-packages' > env-jupyter/.envrc
$ cd env-jupyter
$ direnv allow
$ pip install jupyter
$ pip install spacy
#+end_example

This ~pip install jupyter~ should result in all or at least most packages reporting "Requirement already satisfied" which means the OS provides the actual package files.  OTOH, the ~pip install spacy~ should bring in a mix of OS and fresh packages.  This shows the ~--system-site-packages~ is operational.

The notebook server can then be started by hand:
#+begin_example
  $ python -m ipykernel install --user --name myenv
  $ jupyter notebook --no-browser
  ...
  http://localhost:8888/?token=32c26d0a705495b5a9e55bce6c60f4b7622c45a89fa3fafd
#+end_example
Based on the URL we may create a tunnel:
#+begin_example
$ ssh -L 8888:localhost:8888 wcgpu0.phy.bnl.gov
#+end_example
Now visit the URL in the browser and locate ~myenv~ under ~New->Notebook:~ menu.



* DNNROI
:PROPERTIES:
:CUSTOM_ID: dnnroi
:END:


Prepare:

#+begin_example
$ wget --no-check-certificate -r --no-parent 'https://www.phy.bnl.gov/~hyu/dunefd/dnn-roi-pdvd
/Pytorch-UNet/data/'
$ mv www.phy.bnl.gov/~hyu/dunefd/dnn-roi-pdvd/Pytorch-UNet/data .
$ rm -rf www.phy.bnl.gov/

$ git clone https://github.com/HaiwangYu/Pytorch-UNet.git hyu-Pytorch-UNet
$ echo 'layout python > hyu-Pytorch-UNet/.envrc
$ cd hyu-Pytorch-UNet && direnv allow
$ ln -s ../data .

$ pip install torch torchvision torchaudio matplotlib h5py pillow pandas

$ pip list
Package                  Version
------------------------ -----------
contourpy                1.3.0
cycler                   0.12.1
filelock                 3.16.0
fonttools                4.53.1
fsspec                   2024.9.0
h5py                     3.11.0
Jinja2                   3.1.4
kiwisolver               1.4.7
MarkupSafe               2.1.5
matplotlib               3.9.2
mpmath                   1.3.0
networkx                 3.3
numpy                    2.1.1
nvidia-cublas-cu12       12.1.3.1
nvidia-cuda-cupti-cu12   12.1.105
nvidia-cuda-nvrtc-cu12   12.1.105
nvidia-cuda-runtime-cu12 12.1.105
nvidia-cudnn-cu12        9.1.0.70
nvidia-cufft-cu12        11.0.2.54
nvidia-curand-cu12       10.3.2.106
nvidia-cusolver-cu12     11.4.5.107
nvidia-cusparse-cu12     12.1.0.106
nvidia-nccl-cu12         2.20.5
nvidia-nvjitlink-cu12    12.6.68
nvidia-nvtx-cu12         12.1.105
packaging                24.1
pandas                   2.2.2
pillow                   10.4.0
pip                      23.0.1
pyparsing                3.1.4
python-dateutil          2.9.0.post0
pytz                     2024.2
setuptools               66.1.1
six                      1.16.0
sympy                    1.13.2
torch                    2.4.1
torchaudio               2.4.1
torchvision              0.19.1
triton                   3.0.0
typing_extensions        4.12.2
tzdata                   2024.1
#+end_example

Run:
#+begin_example
$ bash train3.sh

Epoch finished ! Loss: 0.032897
Checkpoint e0 saved !
Traceback (most recent call last):
  File "/home/wcwc/dev/dnnroi/hyu-Pytorch-UNet/train3.py", line 264, in <module>
    train_net(net=net,
  File "/home/wcwc/dev/dnnroi/hyu-Pytorch-UNet/train3.py", line 199, in train_net
    val_loss = eval_loss(net, criterion, val2, gpu)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wcwc/dev/dnnroi/hyu-Pytorch-UNet/eval_util.py", line 43, in eval_loss
    true_masks_flat = true_mask.view(-1)
                      ^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

real	0m28.260s
user	0m26.832s
sys	0m5.687s
#+end_example

Try SBND's flavor

#+begin_example
$ git clone https://github.com/wjdanswjddl/Pytorch-UNet.git moon-Pytorch-UNet

$ echo 'layout python > moon-Pytorch-UNet/.envrc
$ cd moon-Pytorch-UNet && direnv allow
$ ln -s ../data .

$ pip install torch torchvision torchaudio matplotlib h5py pillow pandas
$ pip install tqdm tensorboard

#+end_example



* Attention is all you need :noexport:

One implementation of this famous paper for Pytorch is at [[https://github.com/jadore801120/attention-is-all-you-need-pytorch][Attention is all you need]].  


#+begin_example
git clone https://github.com/jadore801120/attention-is-all-you-need-pytorch.git aiayn
cd aiayn
python -m venv venv
source venv/bin/activate
# overrite totally broken requirements.txt
cat > EOF

EOF
pip install spacy torch torchtext dill
#+end_example
