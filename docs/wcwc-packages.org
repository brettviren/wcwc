#+title: WCWC Packages

* meta :noexport:

#+begin_src sh :results output drawer
scp wcwc-packages.html hierocles.phy.bnl.gov:public_html/wire-cell/docs/
#+end_src

#+RESULTS:
:results:
:end:


* Overview

This note collects information about specific packages.

#+begin_example
wcwc@wcgpu0:~$ export WCWC_PREFIX=$HOME/wcwc-dev
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit@master+torch+cuda cuda_arch=89 target=x86_64
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit@master+torch+cuda+root+hdf+glpk cuda_arch=89 target=x86_64
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-prototype target=x86_64

wcwc@wcgpu0:~$ rsync -av wcwc-dev/  wcwc@lycastus.phy.bnl.gov:/wcwc/
#+end_example


* Architecture target

We use amd threadripper to build which can make binaries that won't work on older intel.  MUST set ~target=x86_64~ qualifier.

** TODO bake this into ~packages.yaml~


* CUDA
:PROPERTIES:
:CUSTOM_ID: cuda
:END:


The main motivation for WCWC is to share workstation GPUs at BNL.   All are currently nVidia and so CUDA support is a must.  nVidia phases out support for old hardware in new software (kernel driver and libraries).  That landscape is complex and described by several things:

- compute capability :: (cc) a number like 9.0 which maps to a set of supported compute features
- hardware family :: eg "turing" is a name that implies one or more cc numbers ([[https://developer.nvidia.com/cuda-gpus][table of device cc]],  [[https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list][table of family cc]])
- driver version :: a number like 525.147.05 which gives the version of the nVidia Linux kernel driver
- toolkit version :: a number like 11.8 or 12.0 which gives the version of CUDA user libraries and implies a minimum driver version.

  #+begin_note
  Debian's ~libcuda~ packages are given a *driver version* number and provide support for multiple *toolkit version* numbers.  Debian 12's ~libcuda1~ package is at version 525.147.05 which provides various libcuda versions in the range 5.0 - 12.0.
  #+end_note

A sampling of BNL GPUs, their amount of RAM and their cc, family and [[https://en.wikipedia.org/wiki/CUDA#GPUs_supported][CUDA toolkit minimum version]] (all are supported up to and including CUDA 12).

|-------------+-------+-----+----------+------|
| Card        | RAM   |  CC | Family   | CUDA |
|-------------+-------+-----+----------+------|
| GTX 750 Ti  | 2 GB  | 5.0 | Maxwell  |  6.5 |
|-------------+-------+-----+----------+------|
| GTX 1050 Ti | 4 GB  | 6.1 | Pascal   |  8.0 |
| GTX 1060    | 6 GB  | 6.1 | Pascal   |      |
|-------------+-------+-----+----------+------|
| GTX 1650    | 4 GB  | 7.4 | Turing   | 10.0 |
| GTX 1660    | 6 GB  | 7.5 | Turing   |      |
| RTX 2080 Ti | 11 GB | 7.5 | Turing   |      |
|-------------+-------+-----+----------+------|
| RTX 3080    | 10 GB | 8.6 | Ampere   | 11.0 |
|-------------+-------+-----+----------+------|
| RTX 4070 Ti | 12 GB | 8.9 | Lovelace | 11.8 |
| RTX 4090    | 24 GB | 8.9 | Lovelace |      |
|-------------+-------+-----+----------+------|


Each workstation must have nVidia kernel driver installed and loaded.  It is
recommended to use Debian's package:
#+begin_example
apt install nvidia-kernel-dkms nvidia-smi
#+end_example
A reboot may be required.  The ~nvidia-smi~ command is used to query your GPU device status.  If it gives an error then the kernel driver is not working.
#+begin_example
wcwc@haiku:~$ nvidia-smi  -L
GPU 0: NVIDIA GeForce GTX 750 Ti (UUID: GPU-b87cc150-ba6d-329e-02e4-7f8ddef6a581)
#+end_example

Spack can compile CUDA from scratch or or use the [[https://spack.readthedocs.io/en/latest/gpu_configuration.html#using-an-external-cuda-installation][system CUDA as an "external" package]] in a custom ~packages.hyaml~ configuration file.  Current Spack ~develop~ branch provides CUDA 11.8.0 and WCWC uses that.
The Spack build of CUDA is relatively quick.

* Torch

PyTorch is the primary DNN toolkit of WCWC users and WCT can make use of libtorch.  Spack's ~py-torch~ package builds the Python interface:

#+begin_example
  $ TMPDIR=$HOME/.cache/wcwc TEMP=$HOME/.cache/wcwc \
    wcwc install "py-torch+cuda cuda_arch=89"
#+end_example
#+begin_note
FIXME: must set ~TMPDIR~ to avoid ~/tmp~ being filled.  For safety, set this and ~TEMP~ in ~wcwc~, but really, this is a bug in the ~py-torch/package.py~ which should honor the configured ~build_stage~ setting.  Note, this is not required when building in a [[file:wcwc-podman.org][podman container]] (assuming podman has sufficient space in the user's home).

Also, earlier builds failed due to OpenMP being detected in the OS but not sufficient to work. 
~apt install libomp-dev~ was needed.  The above was on a different Debian host.  This also motivates the use of a container to build.  In a minimal Debian 12 podman container, py-torch builds without problems (other than loooooong build time).  The ~wcwc make-builder~ command creates the container.
#+end_note

#+begin_example
ls /wcwc/opt/builtin/linux-debian12-x86_64_v4/gcc-12.2.0/py-torch-2.3.0-sq4psktodzbjq7v4bshow7rmcxwoa6r3/lib/python3.11/site-packages/torch/lib                                                                                                          
libc10_cuda.so	libcaffe2_nvrtc.so  libtorch_cpu.so	    libtorch_cuda.so	     libtorch_python.so
libc10.so	libshm.so	   libtorch_cuda_linalg.so  libtorch_global_deps.so  libtorch.so
#+end_example

Simple test:
#+begin_example
  $ python -c 'import torch; print(torch.cuda.is_available())'
  True
  $ python -c 'import torch; print(torch.cuda.device_count())'
  2
#+end_example

Note, had to load a kernel module:

#+begin_example
root@wcgpu0:~# modprobe nvidia_uvm
#+end_example

#+begin_example
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit+torch+cuda cuda_arch=89
#+end_example

* Ollama
:PROPERTIES:
:CUSTOM_ID: ollama
:END:


This is a nightmare package to build.  Instead we add a new ~ollamabin~ Spack packaging in the ~wirecell~ stack.
To use it you must run a server and a client which we do in different shells.

** Server

The server:
#+begin_example
$ wcwc shell -S wirecell -l ollamabin -c "ollama serve"
#+end_example
Note, first time, this unpacks some files into ~$TMPDIR~.  Make sure that points to an area with enough free space.

To control exactly which GPU(s) to use we can set ~CUDA_VISIBLE_DEVICES~ to a comma-separated list of GPU indices or IDs.  Here is an example to use a specific GPU via its ID:
#+begin_example
u$ nvidia-smi -L  # note the GPU IDs
u$ wcwc shell -l ollama+cuda
s$ CUDA_VISIBLE_DEVICES=GPU-7d6529b0-76ce-2e7f-d96e-2018b58d1f2b ollama serve
#+end_example
To stop the server, hit ~Ctrl-c~.

** Client

Run a model via the command line client:
#+begin_example
$ wcwc shell -l ollama+cuda -c "ollama run llama3.1"
#+end_example
This will download several GBs to =~/.ollama/= and then give a chat prompt.  Note, chat is logged by the server.  To exit, hit ~Ctrl-d~.

** Checks
You can run various checks while client+server is running.  Again, start a subshell

#+begin_example
$ wcwc shell -S wirecell -l ollamabin -c "ollama ps"
NAME           	ID          	SIZE  	PROCESSOR	UNTIL                   
llama3.1:latest	f66fc8dc39ea	6.7 GB	100% GPU 	About a minute from now
$ nvidia-smi
...
|    0   N/A  N/A    520242      C   ...unners/cuda_v12/ollama_llama_server       6142MiB |
...
#+end_example

#+begin_note
~ollama ps~ will *lie* to you about using the GPU when it really isn't.  If ~nvidia-smi~ does not show an entry then the ollama is not actually running the model on the GPU.
#+end_note

** hhao / OpenBMB-minicpm-llama3

This model requires a hacked ollama for server and client which is provided by the package ~ollamahhao~.  It may be used like:

#+begin_example
$ wcwc shell -S wirecell -l ollamahhao -c "ollama serve"

$ wcwc shell -S wirecell -l ollamahhao -c "ollama run hhao/openbmb-minicpm-llama3-v-2_5"
#+end_example

