#+title: WCWC Packages
#+setupfile: wcwc-setup.org


* meta :noexport:

#+begin_src sh :results output drawer
scp wcwc-packages.html hierocles.phy.bnl.gov:public_html/wire-cell/docs/
#+end_src

#+RESULTS:
:results:
:end:


* Overview

This note collects information about specific packages.

#+begin_example
wcwc@wcgpu0:~$ export WCWC_PREFIX=$HOME/wcwc-dev
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit@master+torch+cuda cuda_arch=89 target=x86_64
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit@master+torch+cuda+root+hdf+glpk cuda_arch=89 target=x86_64
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-prototype target=x86_64

wcwc@wcgpu0:~$ rsync -av wcwc-dev/  wcwc@lycastus.phy.bnl.gov:/wcwc/
#+end_example


* Architecture target

We use amd threadripper to build which can make binaries that won't work on older intel.  MUST set ~target=x86_64~ qualifier.

** TODO bake this into ~packages.yaml~


* CUDA
:PROPERTIES:
:CUSTOM_ID: cuda
:END:


The main motivation for WCWC is to share workstation GPUs at BNL.   All are currently nVidia and so CUDA support is a must.  nVidia phases out support for old hardware in new software (kernel driver and libraries).  That landscape is complex and described by several things:

- compute capability :: (cc) a number like 9.0 which maps to a set of supported compute features
- hardware family :: eg "turing" is a name that implies one or more cc numbers ([[https://developer.nvidia.com/cuda-gpus][table of device cc]],  [[https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list][table of family cc]])
- driver version :: a number like 525.147.05 which gives the version of the nVidia Linux kernel driver
- toolkit version :: a number like 11.8 or 12.0 which gives the version of CUDA user libraries and implies a minimum driver version.

  #+begin_note
  Debian's ~libcuda~ packages are given a *driver version* number and provide support for multiple *toolkit version* numbers.  Debian 12's ~libcuda1~ package is at version 525.147.05 which provides various libcuda versions in the range 5.0 - 12.0.
  #+end_note

A sampling of BNL GPUs, their amount of RAM and their cc, family and [[https://en.wikipedia.org/wiki/CUDA#GPUs_supported][CUDA toolkit minimum version]] (all are supported up to and including CUDA 12).

|-------------+-------+-----+----------+------|
| Card        | RAM   |  CC | Family   | CUDA |
|-------------+-------+-----+----------+------|
| GTX 750 Ti  | 2 GB  | 5.0 | Maxwell  |  6.5 |
|-------------+-------+-----+----------+------|
| GTX 1050 Ti | 4 GB  | 6.1 | Pascal   |  8.0 |
| GTX 1060    | 6 GB  | 6.1 | Pascal   |      |
|-------------+-------+-----+----------+------|
| GTX 1650    | 4 GB  | 7.4 | Turing   | 10.0 |
| GTX 1660    | 6 GB  | 7.5 | Turing   |      |
| RTX 2080 Ti | 11 GB | 7.5 | Turing   |      |
|-------------+-------+-----+----------+------|
| RTX 3080    | 10 GB | 8.6 | Ampere   | 11.0 |
|-------------+-------+-----+----------+------|
| RTX 4070 Ti | 12 GB | 8.9 | Lovelace | 11.8 |
| RTX 4090    | 24 GB | 8.9 | Lovelace |      |
|-------------+-------+-----+----------+------|


Each workstation must have nVidia kernel driver installed and loaded.  It is
recommended to use Debian's package:
#+begin_example
apt install nvidia-kernel-dkms nvidia-smi
#+end_example
A reboot may be required.  The ~nvidia-smi~ command is used to query your GPU device status.  If it gives an error then the kernel driver is not working.
#+begin_example
wcwc@haiku:~$ nvidia-smi  -L
GPU 0: NVIDIA GeForce GTX 750 Ti (UUID: GPU-b87cc150-ba6d-329e-02e4-7f8ddef6a581)
#+end_example

Spack can compile CUDA from scratch or or use the [[https://spack.readthedocs.io/en/latest/gpu_configuration.html#using-an-external-cuda-installation][system CUDA as an "external" package]] in a custom ~packages.hyaml~ configuration file.  Current Spack ~develop~ branch provides CUDA 11.8.0 and WCWC uses that.
The Spack build of CUDA is relatively quick.

* Torch
:PROPERTIES:
:CUSTOM_ID: torch
:END:


PyTorch is the primary DNN toolkit of WCWC users and WCT can make use of libtorch.  Spack's ~py-torch~ package builds the Python interface:

#+begin_example
  $ TMPDIR=$HOME/.cache/wcwc TEMP=$HOME/.cache/wcwc \
    wcwc install "py-torch+cuda cuda_arch=89"
#+end_example
#+begin_note
FIXME: must set ~TMPDIR~ to avoid ~/tmp~ being filled.  For safety, set this and ~TEMP~ in ~wcwc~, but really, this is a bug in the ~py-torch/package.py~ which should honor the configured ~build_stage~ setting.  Note, this is not required when building in a [[file:wcwc-podman.org][podman container]] (assuming podman has sufficient space in the user's home).

Also, earlier builds failed due to OpenMP being detected in the OS but not sufficient to work. 
~apt install libomp-dev~ was needed.  The above was on a different Debian host.  This also motivates the use of a container to build.  In a minimal Debian 12 podman container, py-torch builds without problems (other than loooooong build time).  The ~wcwc make-builder~ command creates the container.
#+end_note

#+begin_example
ls /wcwc/opt/builtin/linux-debian12-x86_64_v4/gcc-12.2.0/py-torch-2.3.0-sq4psktodzbjq7v4bshow7rmcxwoa6r3/lib/python3.11/site-packages/torch/lib                                                                                                          
libc10_cuda.so	libcaffe2_nvrtc.so  libtorch_cpu.so	    libtorch_cuda.so	     libtorch_python.so
libc10.so	libshm.so	   libtorch_cuda_linalg.so  libtorch_global_deps.so  libtorch.so
#+end_example

Simple test:
#+begin_example
  $ python -c 'import torch; print(torch.cuda.is_available())'
  True
  $ python -c 'import torch; print(torch.cuda.device_count())'
  2
#+end_example

Note, had to load a kernel module:

#+begin_example
root@wcgpu0:~# modprobe nvidia_uvm
#+end_example

#+begin_example
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit+torch+cuda cuda_arch=89
#+end_example

* Ollama
:PROPERTIES:
:CUSTOM_ID: ollama
:END:

The Ollama project provides the program ~ollama~ that exercises LLM models.

** Versions
:PROPERTIES:
:CUSTOM_ID: ollama-versions
:END:


The official [[https://github.com/ollama/ollama/][ollama/ollama]] supports a number of different LLM [[https://github.com/ollama/ollama/?tab=readme-ov-file#model-library][models]].  Some other models of interest require a modified ~ollama~ program.  These variants may not support the same models as the official ~ollama~.  In general, one must only use models that are explicitly supported by a particular ~ollama~ variant.

WCWC provides official and variant ~ollama~ as different Spack packages.  Currently:

- ollamabin :: The official version.  The "bin" implies the package consists of a [[https://github.com/ollama/ollama/releases][binary release]] and to avoid conflict with the Spack ~ollama~ package that builds from source (and which is out of date and does build against CUDA, but see [[https://github.com/spack/spack/pull/46204][spack#46204]])
- ollamalux :: A "hacked" version to run specifically ~aiden_lu/minicpm-v2.6~.  The "lux" portion of the name refers to it originating from [[https://github.com/luixiao0/ollama/releases][luixiao0/ollama binary release]].



Regardless of the package, the user uses the program named ~ollama~.

** Environment
:PROPERTIES:
:CUSTOM_ID: ollama-env
:END:

To help keep the multiple ~ollama~ variants distinct, the WCWC packages set a number of environment variables. 

- ~OLLAMA_HOST~ :: server IP address and port number.
- ~OLLAMA_TMPDIR~ :: set to ~$HOME/tmp~ to avoid filling ~/tmp~
- ~OLLAMA_MODELS~ :: location to store files for models.

#+begin_note
For other available environment variables, run ~ollama serve --help~.
#+end_note

Multiple clients, possibly run by multiple users, may share the same server.  The user that starts the server may also kill it which of course will impact all clients.  Users are encouraged to coordinate any sharing amongst themselves.  A user may attempt to isolate their server by setting ~OLLAMA_HOST~.

#+begin_note
Users should not leave ~ollama~ server running longer than needed as it consumes GPU memory.
#+end_note

For either server or command line client usage, start a shell like:
#+begin_example
u$ wcwc shell -S wirecell -l ollamaXXX
s$ ollama [...]
#+end_example

Replace ~ollamaXXX~ with one of the package names in the list above.




** Server
:PROPERTIES:
:CUSTOM_ID: ollama-server
:END:

The ~ollama~ server is started simply:
#+begin_example
$ ollama serve
#+end_example
To stop the server, simply hit ~Ctrl-c~.

By default, ~ollama~ will locate GPU(s) and use them as it sees suited.  To control which GPU(s) it should use:
#+begin_example
$ nvidia-smi -L  # note the GPU IDs
$ CUDA_VISIBLE_DEVICES=GPU-7d6529b0-76ce-2e7f-d96e-2018b58d1f2b ollama serve
#+end_example


** Client
:PROPERTIES:
:CUSTOM_ID: ollama-client
:END:

The ~ollama~ program can also act as a command line client:
#+begin_example
$ ollama run llama3.1
#+end_example
Chat is logged by the server.  To exit, hit ~Ctrl-d~.

The first time a model is run its files will be downloaded by the server.  Before starting the server, the location can be controlled by ~OLLAMA_MODELS~ which WCWC ~ollamaXXX~ package sets to ~$HOME/.cache/ollama/models/XXX/~.


** Checks
:PROPERTIES:
:CUSTOM_ID: ollama-checks
:END:

You can run various checks to observe the state of the server.  Assuming a subshell configured for the ~ollamaXXX~ package:

#+begin_example
$ ollama ls  # list available models
NAME           	ID          	SIZE  	MODIFIED       
llama3.1:latest	f66fc8dc39ea	4.7 GB	42 seconds ago	

$ ollama ps  # list running models:
NAME           	ID          	SIZE  	PROCESSOR	UNTIL              
llama3.1:latest	f66fc8dc39ea	6.7 GB	100% GPU 	4 minutes from now	

$ nvidia-smi # check actual GPU memory usage
...
|    0   N/A  N/A    531064      C   ...unners/cuda_v12/ollama_llama_server       6142MiB |
...
#+end_example

** Specific models
:PROPERTIES:
:CUSTOM_ID: ollama-variants
:END:

Above we showed examples with the official ~ollama~ program and supported models.  This section collects guidance on variants.  The models described below should not be expected to work with the official ~ollama~ nor should the ~ollama~'s described her be expected to run the officially supported models.

*** lux / MiniCPM v2.6
:PROPERTIES:
:CUSTOM_ID: ollama-lux
:END:

The MiniCPM v2.6 model is claimed to be good for OCR.  It requires a variant ~ollama~ which is provided by the ~ollamalux~ package.  It may be used from two shells like:

#+begin_example
$ wcwc shell -S wirecell -l ollamalux -c "ollama serve"

$ wcwc shell -S wirecell -l ollamalux -c "ollama run aiden_lu/minicpm-v2.6:Q4_K_M"
#+end_example

