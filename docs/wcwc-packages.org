#+title: WCWC Packages

This note collects information about specific packages.

#+begin_example
wcwc@wcgpu0:~$ export WCWC_PREFIX=$HOME/wcwc-dev
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit@master+torch+cuda cuda_arch=89 target=x86_64
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit@master+torch+cuda+root+hdf+glpk cuda_arch=89 target=x86_64
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-prototype target=x86_64

wcwc@wcgpu0:~$ rsync -av wcwc-dev/  wcwc@lycastus.phy.bnl.gov:/wcwc/
#+end_example


* Architecture target

We use amd threadripper to build which can make binaries that won't work on older intel.  MUST set ~target=x86_64~ qualifier.

** TODO bake this into ~packages.yaml~


* CUDA
:PROPERTIES:
:CUSTOM_ID: cuda
:END:


The main motivation for WCWC is to share workstation GPUs at BNL.   All are currently nVidia and so CUDA support is a must.  nVidia phases out support for old hardware in new software (kernel driver and libraries).  That landscape is complex and described by several things:

- compute capability :: (cc) a number like 9.0 which maps to a set of supported compute features
- hardware family :: eg "turing" is a name that implies one or more cc numbers ([[https://developer.nvidia.com/cuda-gpus][table of device cc]],  [[https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list][table of family cc]])
- driver version :: a number like 525.147.05 which gives the version of the nVidia Linux kernel driver
- toolkit version :: a number like 11.8 or 12.0 which gives the version of CUDA user libraries and implies a minimum driver version.

  #+begin_note
  Debian's ~libcuda~ packages are given a *driver version* number and provide support for multiple *toolkit version* numbers.  Debian 12's ~libcuda1~ package is at version 525.147.05 which provides various libcuda versions in the range 5.0 - 12.0.
  #+end_note

A sampling of BNL GPUs, their amount of RAM and their cc, family and [[https://en.wikipedia.org/wiki/CUDA#GPUs_supported][CUDA toolkit minimum version]] (all are supported up to and including CUDA 12).

|-------------+-------+-----+----------+------|
| Card        | RAM   |  CC | Family   | CUDA |
|-------------+-------+-----+----------+------|
| GTX 750 Ti  | 2 GB  | 5.0 | Maxwell  |  6.5 |
|-------------+-------+-----+----------+------|
| GTX 1050 Ti | 4 GB  | 6.1 | Pascal   |  8.0 |
| GTX 1060    | 6 GB  | 6.1 | Pascal   |      |
|-------------+-------+-----+----------+------|
| GTX 1650    | 4 GB  | 7.4 | Turing   | 10.0 |
| GTX 1660    | 6 GB  | 7.5 | Turing   |      |
| RTX 2080 Ti | 11 GB | 7.5 | Turing   |      |
|-------------+-------+-----+----------+------|
| RTX 3080    | 10 GB | 8.6 | Ampere   | 11.0 |
|-------------+-------+-----+----------+------|
| RTX 4070 Ti | 12 GB | 8.9 | Lovelace | 11.8 |
| RTX 4090    | 24 GB | 8.9 | Lovelace |      |
|-------------+-------+-----+----------+------|


Each workstation must have nVidia kernel driver installed and loaded.  It is
recommended to use Debian's package:
#+begin_example
apt install nvidia-kernel-dkms nvidia-smi
#+end_example
A reboot may be required.  The ~nvidia-smi~ command is used to query your GPU device status.  If it gives an error then the kernel driver is not working.
#+begin_example
wcwc@haiku:~$ nvidia-smi  -L
GPU 0: NVIDIA GeForce GTX 750 Ti (UUID: GPU-b87cc150-ba6d-329e-02e4-7f8ddef6a581)
#+end_example

Spack can compile CUDA from scratch or or use the [[https://spack.readthedocs.io/en/latest/gpu_configuration.html#using-an-external-cuda-installation][system CUDA as an "external" package]] in a custom ~packages.hyaml~ configuration file.  Current Spack ~develop~ branch provides CUDA 11.8.0 and WCWC uses that.
The Spack build of CUDA is relatively quick.

* Torch

PyTorch is the primary DNN toolkit of WCWC users and WCT can make use of libtorch.  Spack's ~py-torch~ package builds the Python interface:

#+begin_example
  $ TMPDIR=$HOME/.cache/wcwc TEMP=$HOME/.cache/wcwc \
    wcwc install "py-torch+cuda cuda_arch=89"
#+end_example
#+begin_note
FIXME: must set ~TMPDIR~ to avoid ~/tmp~ being filled.  For safety, set this and ~TEMP~ in ~wcwc~, but really, this is a bug in the ~py-torch/package.py~ which should honor the configured ~build_stage~ setting.  Note, this is not required when building in a [[file:wcwc-podman.org][podman container]] (assuming podman has sufficient space in the user's home).

Also, earlier builds failed due to OpenMP being detected in the OS but not sufficient to work. 
~apt install libomp-dev~ was needed.  The above was on a different Debian host.  This also motivates the use of a container to build.  In a minimal Debian 12 podman container, py-torch builds without problems (other than loooooong build time).  The ~wcwc make-builder~ command creates the container.
#+end_note

#+begin_example
ls /wcwc/opt/builtin/linux-debian12-x86_64_v4/gcc-12.2.0/py-torch-2.3.0-sq4psktodzbjq7v4bshow7rmcxwoa6r3/lib/python3.11/site-packages/torch/lib                                                                                                          
libc10_cuda.so	libcaffe2_nvrtc.so  libtorch_cpu.so	    libtorch_cuda.so	     libtorch_python.so
libc10.so	libshm.so	   libtorch_cuda_linalg.so  libtorch_global_deps.so  libtorch.so
#+end_example

Simple test:
#+begin_example
  $ python -c 'import torch; print(torch.cuda.is_available())'
  True
  $ python -c 'import torch; print(torch.cuda.device_count())'
  2
#+end_example

Note, had to load a kernel module:

#+begin_example
root@wcgpu0:~# modprobe nvidia_uvm
#+end_example

#+begin_example
wcwc@wcgpu0:~$ wcwc admin install -S wirecell wire-cell-toolkit+torch+cuda cuda_arch=89
#+end_example

* Ollama

The packaging for ~ollama~ in Spack was last updated for version 0.1.31 and contains no support for building against CUDA.  Meanwhile the native "installation" method is the dreaded ~curl URL | sh~.  The piped script is not at all suitable for use on WCWC.  So, we shall update the ~ollama~ Spack package.

In doing that we want to immediately exercise it to build packages on WCWC but also to be able to offer a PR to Spack.  This means developing ~ollama/package.py~ in a copy of Spack and temporarily placing a copy into ~wire-cell-spack~.

First, start with my fork which was last touched 7 years ago and find humorous to see:
#+begin_example
This branch is 21 commits ahead of, 29446 commits behind spack/spack:develop.
#+end_example
None of those 21 commits are useful today.

#+begin_example
$ ssh wcwc@wcgpu0.phy.bnl.gov
$ git clone git@github.com:brettviren/spack.git ~/dev/spack-ollama
$ source ~/dev/spack-ollama/share/spack/setup-env.sh
$ spack checksum ollama
==> Error: Could not find any remote versions for ollama
$ spack edit ollama
$ spack checksum ollama
(paste version line into open package.py)
#+end_example
Edits:
- Add ~url~ parameter pointing to recent source archive on GitHub.
- Add ~version("0.3.9")~ line from ~spack checksum~.
- Remove build dependencies on ccache, git and gcc (there is now a generated dependency on "cxx").
- Add minimum version constraints on cmake and go.
Blithely try an as-is install.
#+begin_example
$ spack install ollama
#+end_example
Note, this is installing zen3 arch because I forgot to set the ~target~.  Once things are building, I'll redo via wcwc and container.

Next, extend package.py to add support for building against CUDA referring to [[https://github.com/ollama/ollama/blob/main/docs/development.md][this doc]].   

Environment or  variables:
- ~CUDACXX~ path for ~nvcc~.
- ~CUDA_LIB_DIR~ location of CUDA shared libraries.
And, this cmake directive:
- ~CMAKE_CUDA_ARCHITECTURES~ eg to ~50;60;70;~

Apparently, a lot of build logic is in this mess: [[https://github.com/ollama/ollama/blob/main/llm/generate/gen_linux.sh][gen_linux.sh]].  It is invoked from their ~Dockerfile~.  It seems to be the consumer of ~CUDA_LIB_DIR~ and ~CUDACXX~ so likely must be considered.
